{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad97b0-f3eb-468e-9660-e1626dad825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.923245  [    0/ 2908]\n",
      "loss: 2.170461  [  100/ 2908]\n",
      "loss: 1.985539  [  200/ 2908]\n",
      "loss: 2.139325  [  300/ 2908]\n",
      "loss: 1.999366  [  400/ 2908]\n",
      "loss: 2.146954  [  500/ 2908]\n",
      "loss: 2.095107  [  600/ 2908]\n",
      "loss: 2.057983  [  700/ 2908]\n",
      "loss: 2.253879  [  800/ 2908]\n",
      "loss: 2.143314  [  900/ 2908]\n",
      "loss: 2.162476  [ 1000/ 2908]\n",
      "loss: 2.127250  [ 1100/ 2908]\n",
      "loss: 2.039821  [ 1200/ 2908]\n",
      "loss: 2.044947  [ 1300/ 2908]\n",
      "loss: 2.027104  [ 1400/ 2908]\n",
      "loss: 2.164116  [ 1500/ 2908]\n",
      "loss: 2.040317  [ 1600/ 2908]\n",
      "loss: 2.023619  [ 1700/ 2908]\n",
      "loss: 2.200508  [ 1800/ 2908]\n",
      "loss: 2.027173  [ 1900/ 2908]\n",
      "loss: 2.020463  [ 2000/ 2908]\n",
      "loss: 2.211768  [ 2100/ 2908]\n",
      "loss: 2.045523  [ 2200/ 2908]\n",
      "loss: 1.982244  [ 2300/ 2908]\n",
      "loss: 2.202669  [ 2400/ 2908]\n",
      "loss: 1.968250  [ 2500/ 2908]\n",
      "loss: 2.087827  [ 2600/ 2908]\n",
      "loss: 1.998955  [ 2700/ 2908]\n",
      "loss: 1.991636  [ 2800/ 2908]\n",
      "loss: 2.214566  [ 2320/ 2908]\n",
      "Avg loss: 2.330754 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.081328  [    0/ 2908]\n",
      "loss: 2.029533  [  100/ 2908]\n",
      "loss: 2.057186  [  200/ 2908]\n",
      "loss: 1.998753  [  300/ 2908]\n",
      "loss: 2.069468  [  400/ 2908]\n",
      "loss: 2.059427  [  500/ 2908]\n",
      "loss: 2.073246  [  600/ 2908]\n",
      "loss: 1.941238  [  700/ 2908]\n",
      "loss: 1.940316  [  800/ 2908]\n",
      "loss: 2.153870  [  900/ 2908]\n",
      "loss: 2.029523  [ 1000/ 2908]\n",
      "loss: 1.918235  [ 1100/ 2908]\n",
      "loss: 2.079867  [ 1200/ 2908]\n",
      "loss: 1.981545  [ 1300/ 2908]\n",
      "loss: 2.156997  [ 1400/ 2908]\n",
      "loss: 1.958577  [ 1500/ 2908]\n",
      "loss: 1.982787  [ 1600/ 2908]\n",
      "loss: 1.875218  [ 1700/ 2908]\n",
      "loss: 2.034335  [ 1800/ 2908]\n",
      "loss: 1.999276  [ 1900/ 2908]\n",
      "loss: 2.091883  [ 2000/ 2908]\n",
      "loss: 1.994542  [ 2100/ 2908]\n",
      "loss: 1.996898  [ 2200/ 2908]\n",
      "loss: 1.922447  [ 2300/ 2908]\n",
      "loss: 2.079235  [ 2400/ 2908]\n",
      "loss: 2.015292  [ 2500/ 2908]\n",
      "loss: 2.088802  [ 2600/ 2908]\n",
      "loss: 1.887573  [ 2700/ 2908]\n",
      "loss: 1.845460  [ 2800/ 2908]\n",
      "loss: 1.899907  [ 2320/ 2908]\n",
      "Avg loss: 2.124112 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.837544  [    0/ 2908]\n",
      "loss: 2.078777  [  100/ 2908]\n",
      "loss: 2.084924  [  200/ 2908]\n",
      "loss: 1.965151  [  300/ 2908]\n",
      "loss: 1.872140  [  400/ 2908]\n",
      "loss: 1.894595  [  500/ 2908]\n",
      "loss: 1.727163  [  600/ 2908]\n",
      "loss: 1.931962  [  700/ 2908]\n",
      "loss: 1.767900  [  800/ 2908]\n",
      "loss: 1.906164  [  900/ 2908]\n",
      "loss: 1.753236  [ 1000/ 2908]\n",
      "loss: 1.944558  [ 1100/ 2908]\n",
      "loss: 1.814824  [ 1200/ 2908]\n",
      "loss: 1.764130  [ 1300/ 2908]\n",
      "loss: 1.782692  [ 1400/ 2908]\n",
      "loss: 1.861254  [ 1500/ 2908]\n",
      "loss: 2.024124  [ 1600/ 2908]\n",
      "loss: 1.983401  [ 1700/ 2908]\n",
      "loss: 2.006080  [ 1800/ 2908]\n",
      "loss: 1.999564  [ 1900/ 2908]\n",
      "loss: 1.832207  [ 2000/ 2908]\n",
      "loss: 1.836661  [ 2100/ 2908]\n",
      "loss: 1.978675  [ 2200/ 2908]\n",
      "loss: 2.083246  [ 2300/ 2908]\n",
      "loss: 2.035208  [ 2400/ 2908]\n",
      "loss: 1.713560  [ 2500/ 2908]\n",
      "loss: 1.910914  [ 2600/ 2908]\n",
      "loss: 1.891510  [ 2700/ 2908]\n",
      "loss: 1.787941  [ 2800/ 2908]\n",
      "loss: 1.738327  [ 2320/ 2908]\n",
      "Avg loss: 2.119349 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.023272  [    0/ 2908]\n",
      "loss: 1.900037  [  100/ 2908]\n",
      "loss: 1.937137  [  200/ 2908]\n",
      "loss: 1.800647  [  300/ 2908]\n",
      "loss: 1.836155  [  400/ 2908]\n",
      "loss: 1.918384  [  500/ 2908]\n",
      "loss: 2.065769  [  600/ 2908]\n",
      "loss: 1.954606  [  700/ 2908]\n",
      "loss: 1.782391  [  800/ 2908]\n",
      "loss: 1.964286  [  900/ 2908]\n",
      "loss: 1.863565  [ 1000/ 2908]\n",
      "loss: 1.904411  [ 1100/ 2908]\n",
      "loss: 1.941050  [ 1200/ 2908]\n",
      "loss: 1.928971  [ 1300/ 2908]\n",
      "loss: 1.913341  [ 1400/ 2908]\n",
      "loss: 1.881287  [ 1500/ 2908]\n",
      "loss: 1.921457  [ 1600/ 2908]\n",
      "loss: 1.860221  [ 1700/ 2908]\n",
      "loss: 1.718752  [ 1800/ 2908]\n",
      "loss: 1.818316  [ 1900/ 2908]\n",
      "loss: 1.757354  [ 2000/ 2908]\n",
      "loss: 1.774314  [ 2100/ 2908]\n",
      "loss: 1.773153  [ 2200/ 2908]\n",
      "loss: 1.778236  [ 2300/ 2908]\n",
      "loss: 1.756021  [ 2400/ 2908]\n",
      "loss: 1.904763  [ 2500/ 2908]\n",
      "loss: 1.960237  [ 2600/ 2908]\n",
      "loss: 1.819521  [ 2700/ 2908]\n",
      "loss: 1.759081  [ 2800/ 2908]\n",
      "loss: 1.833826  [ 2320/ 2908]\n",
      "Avg loss: 2.099194 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.818640  [    0/ 2908]\n",
      "loss: 1.699689  [  100/ 2908]\n",
      "loss: 1.821447  [  200/ 2908]\n",
      "loss: 1.835947  [  300/ 2908]\n",
      "loss: 1.793380  [  400/ 2908]\n",
      "loss: 1.885815  [  500/ 2908]\n",
      "loss: 1.987295  [  600/ 2908]\n",
      "loss: 1.733419  [  700/ 2908]\n",
      "loss: 1.940797  [  800/ 2908]\n",
      "loss: 1.689612  [  900/ 2908]\n",
      "loss: 1.832579  [ 1000/ 2908]\n",
      "loss: 1.888208  [ 1100/ 2908]\n",
      "loss: 1.838759  [ 1200/ 2908]\n",
      "loss: 1.755082  [ 1300/ 2908]\n",
      "loss: 1.901238  [ 1400/ 2908]\n",
      "loss: 1.734726  [ 1500/ 2908]\n",
      "loss: 1.900757  [ 1600/ 2908]\n",
      "loss: 1.746506  [ 1700/ 2908]\n",
      "loss: 1.775125  [ 1800/ 2908]\n",
      "loss: 1.875581  [ 1900/ 2908]\n",
      "loss: 1.725088  [ 2000/ 2908]\n",
      "loss: 1.855099  [ 2100/ 2908]\n",
      "loss: 1.783758  [ 2200/ 2908]\n",
      "loss: 1.797367  [ 2300/ 2908]\n",
      "loss: 1.723282  [ 2400/ 2908]\n",
      "loss: 1.796953  [ 2500/ 2908]\n",
      "loss: 1.647143  [ 2600/ 2908]\n",
      "loss: 1.612495  [ 2700/ 2908]\n",
      "loss: 1.728648  [ 2800/ 2908]\n",
      "loss: 1.563721  [ 2320/ 2908]\n",
      "Avg loss: 2.069909 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.760876  [    0/ 2908]\n",
      "loss: 1.778675  [  100/ 2908]\n",
      "loss: 1.668006  [  200/ 2908]\n",
      "loss: 1.849380  [  300/ 2908]\n",
      "loss: 1.862600  [  400/ 2908]\n",
      "loss: 1.985968  [  500/ 2908]\n",
      "loss: 1.588637  [  600/ 2908]\n",
      "loss: 1.871513  [  700/ 2908]\n",
      "loss: 1.727757  [  800/ 2908]\n",
      "loss: 1.749858  [  900/ 2908]\n",
      "loss: 1.804736  [ 1000/ 2908]\n",
      "loss: 1.749725  [ 1100/ 2908]\n",
      "loss: 1.731876  [ 1200/ 2908]\n",
      "loss: 1.719256  [ 1300/ 2908]\n",
      "loss: 1.663939  [ 1400/ 2908]\n",
      "loss: 1.726227  [ 1500/ 2908]\n",
      "loss: 1.686804  [ 1600/ 2908]\n",
      "loss: 1.612812  [ 1700/ 2908]\n",
      "loss: 1.596283  [ 1800/ 2908]\n",
      "loss: 1.776894  [ 1900/ 2908]\n",
      "loss: 1.681737  [ 2000/ 2908]\n",
      "loss: 1.687873  [ 2100/ 2908]\n",
      "loss: 1.710378  [ 2200/ 2908]\n",
      "loss: 1.648736  [ 2300/ 2908]\n",
      "loss: 1.748824  [ 2400/ 2908]\n",
      "loss: 1.470507  [ 2500/ 2908]\n",
      "loss: 1.760603  [ 2600/ 2908]\n",
      "loss: 1.587116  [ 2700/ 2908]\n",
      "loss: 1.625393  [ 2800/ 2908]\n",
      "loss: 1.820023  [ 2320/ 2908]\n",
      "Avg loss: 2.084339 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.608015  [    0/ 2908]\n",
      "loss: 1.747148  [  100/ 2908]\n",
      "loss: 1.746590  [  200/ 2908]\n",
      "loss: 1.885065  [  300/ 2908]\n",
      "loss: 1.715018  [  400/ 2908]\n",
      "loss: 1.747887  [  500/ 2908]\n",
      "loss: 1.698853  [  600/ 2908]\n",
      "loss: 1.781396  [  700/ 2908]\n",
      "loss: 1.642246  [  800/ 2908]\n",
      "loss: 1.580996  [  900/ 2908]\n",
      "loss: 1.772795  [ 1000/ 2908]\n",
      "loss: 1.750494  [ 1100/ 2908]\n",
      "loss: 1.646885  [ 1200/ 2908]\n",
      "loss: 1.602919  [ 1300/ 2908]\n",
      "loss: 1.763529  [ 1400/ 2908]\n",
      "loss: 1.519951  [ 1500/ 2908]\n",
      "loss: 1.723641  [ 1600/ 2908]\n",
      "loss: 1.642615  [ 1700/ 2908]\n",
      "loss: 1.648648  [ 1800/ 2908]\n",
      "loss: 1.627181  [ 1900/ 2908]\n",
      "loss: 1.715174  [ 2000/ 2908]\n",
      "loss: 1.726399  [ 2100/ 2908]\n",
      "loss: 1.719702  [ 2200/ 2908]\n",
      "loss: 1.483448  [ 2300/ 2908]\n",
      "loss: 1.730608  [ 2400/ 2908]\n",
      "loss: 1.703137  [ 2500/ 2908]\n",
      "loss: 1.565661  [ 2600/ 2908]\n",
      "loss: 1.702423  [ 2700/ 2908]\n",
      "loss: 1.610245  [ 2800/ 2908]\n",
      "loss: 1.574726  [ 2320/ 2908]\n",
      "Avg loss: 1.975455 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.745985  [    0/ 2908]\n",
      "loss: 1.669028  [  100/ 2908]\n",
      "loss: 1.539255  [  200/ 2908]\n",
      "loss: 1.602352  [  300/ 2908]\n",
      "loss: 1.661067  [  400/ 2908]\n",
      "loss: 1.408545  [  500/ 2908]\n",
      "loss: 1.665642  [  600/ 2908]\n",
      "loss: 1.657305  [  700/ 2908]\n",
      "loss: 1.577912  [  800/ 2908]\n",
      "loss: 1.656545  [  900/ 2908]\n",
      "loss: 1.529141  [ 1000/ 2908]\n",
      "loss: 1.568061  [ 1100/ 2908]\n",
      "loss: 1.511673  [ 1200/ 2908]\n",
      "loss: 1.481386  [ 1300/ 2908]\n",
      "loss: 1.661866  [ 1400/ 2908]\n",
      "loss: 1.639156  [ 1500/ 2908]\n",
      "loss: 1.653268  [ 1600/ 2908]\n",
      "loss: 1.702008  [ 1700/ 2908]\n",
      "loss: 1.711960  [ 1800/ 2908]\n",
      "loss: 1.620874  [ 1900/ 2908]\n",
      "loss: 1.467137  [ 2000/ 2908]\n",
      "loss: 1.494675  [ 2100/ 2908]\n",
      "loss: 1.925342  [ 2200/ 2908]\n",
      "loss: 1.613668  [ 2300/ 2908]\n",
      "loss: 1.607219  [ 2400/ 2908]\n",
      "loss: 1.518698  [ 2500/ 2908]\n",
      "loss: 1.398560  [ 2600/ 2908]\n",
      "loss: 1.459639  [ 2700/ 2908]\n",
      "loss: 1.555698  [ 2800/ 2908]\n"
     ]
    }
   ],
   "source": [
    "from DeepLab_V3 import Deeplab_v3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from Datasource import train_dataloader,test_dataloader\n",
    "\n",
    "model = Deeplab_v3()\n",
    "model.load_state_dict(torch.load('model_weights_10.pth'))\n",
    "model.train()\n",
    "model = model.cuda()\n",
    "learning_rate = 1e-2\n",
    "epochs = 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "#功力还不太够，先按照pytorch tutorials的流程来仿照着写\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    #print(len(test_dataloader.dataset))\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    torch.save(model.state_dict(), 'model_weights_11.pth')\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
